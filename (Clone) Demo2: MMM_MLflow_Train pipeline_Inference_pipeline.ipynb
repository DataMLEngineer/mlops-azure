{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aba68cc7-fa55-44de-9bfc-28f115a8c750",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ML FLOW\n",
    "# Capabilities of MLflow\n",
    "\n",
    "There are four components to MLflow:\n",
    "\n",
    "- MLflow Tracking\n",
    "- MLflow Projects\n",
    "- MLflow Models\n",
    "- MLflow Model Registry\n",
    "### MLflow Tracking\n",
    "MLflow Tracking allows data scientists to work with experiments in which they process and analyze data or train machine learning models. For each run in an experiment, a data scientist can log parameter values, versions of libraries used, model evaluation metrics, and generated output files; including images of data visualizations and model files. This ability to log important details about experiment runs makes it possible to audit and compare the results of prior model training executions.\n",
    "\n",
    "### MLflow Projects\n",
    "An MLflow Project is a way of packaging up code for consistent deployment and reproducibility of results. MLflow supports several environments for projects, including the use of Conda and Docker to define consistent Python code execution environments.\n",
    "\n",
    "### MLflow Models\n",
    "MLflow offers a standardized format for packaging models for distribution. This standardized model format allows MLflow to work with models generated from several popular libraries, including Scikit-Learn, PyTorch, MLlib, and others.\n",
    "\n",
    "### MLflow Model Registry\n",
    "The MLflow Model Registry allows data scientists to register trained models. MLflow Models and MLflow Projects use the MLflow Model Registry to enable machine learning engineers to deploy and serve models for client applications to consume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7eabb27-b422-4661-b1a8-6de293ab9a20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " %sh\n",
    " rm -r /dbfs/mlflow_lab\n",
    " mkdir /dbfs/mlflow_lab\n",
    " wget -O /dbfs/mlflow_lab/MMM.csv https://raw.githubusercontent.com/sruthidasrg/test_databrick/467d848ec975a4e07a1505800b7413e74bf2a516/MMM.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85a60d50-f7a0-4a75-b467-dc3b707a60fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Training pipeline /Model Evalution pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6b11c1a-31cb-484b-9560-8ee033b578cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "   \n",
    "data = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"dbfs:/mlflow_lab/MMM.csv\")\n",
    "\n",
    "# List of columns to convert\n",
    "columns_to_convert = [\"TV_Spend\", \"Digital_Spend\", \"Social_Media_Spend\",\"Print_Spend\",\"Discount_Applied\",\"Units_Sold\"]\n",
    "# Apply casting to each column\n",
    "for col_name in columns_to_convert:\n",
    "    data = data.withColumn(col_name, col(col_name).cast(\"long\"))\n",
    "data = data.withColumn(\"Sales\", col(\"Sales\").cast(\"double\"))\n",
    "data = data.withColumn(\"Date\", col(\"Date\").cast(\"date\"))\n",
    "\n",
    "display(data.sample(0.2))\n",
    "   \n",
    "splits = data.randomSplit([0.7, 0.3])\n",
    "train = splits[0]\n",
    "test = splits[1]\n",
    "print (\"Training Rows:\", train.count(), \" Testing Rows:\", test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d117afd4-c9df-4279-866d-28fdb220d3d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6c3d218-45dc-422b-b833-c90f508affdb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def train_market_media_mix_model(training_data, test_data, maxIterations, regularization):\n",
    "    import mlflow\n",
    "    import mlflow.spark\n",
    "    from pyspark.ml import Pipeline\n",
    "    from pyspark.ml.feature import StringIndexer, VectorAssembler, MinMaxScaler,StringIndexer\n",
    "    from pyspark.ml.regression import LinearRegression\n",
    "    from pyspark.ml.evaluation import RegressionEvaluator\n",
    "    import time\n",
    "\n",
    "    # Start an MLflow run\n",
    "    mlflow.set_experiment(\"/Users/sruthidas.g@einfochipsindia.onmicrosoft.com/my-experimentdb_train\")\n",
    "    with mlflow.start_run() as run:\n",
    "        run_id = run.info.run_id \n",
    "        # parameters\n",
    "        maxIterations = 5\n",
    "        regularization = 0.5\n",
    "\n",
    "        catFeatures = ['Campaign_Type', 'Product_Type', 'Region']\n",
    "        numFeatures=['TV_Spend', 'Digital_Spend', 'Social_Media_Spend', 'Print_Spend', 'Discount_Applied']\n",
    "\n",
    "        catIndexers = [StringIndexer(inputCol=col, outputCol=col + \"Idx\", handleInvalid=\"keep\") for col in catFeatures]\n",
    "\n",
    "        numVector = VectorAssembler(inputCols=numFeatures, outputCol=\"numericFeatures\")\n",
    "        numScaler = MinMaxScaler(inputCol = numVector.getOutputCol(), outputCol=\"normalizedFeatures\")\n",
    "        allFeatures = [col + \"Idx\" for col in catFeatures] + [\"normalizedFeatures\"]\n",
    "        featureVector = VectorAssembler(inputCols=allFeatures, outputCol=\"features\")\n",
    "        \n",
    "        # Define the Linear Regression model with hyperparameters\n",
    "        algo = LinearRegression(\n",
    "            labelCol=\"Sales\",  # Ensure this is the correct target variable\n",
    "            featuresCol=\"features\",\n",
    "            maxIter=maxIterations,\n",
    "            regParam=regularization\n",
    "        )\n",
    "\n",
    "        # lr = LinearRegression(featuresCol='features', labelCol='Sales', predictionCol='prediction')\n",
    "        # Chain the steps as stages in a pipeline\n",
    "        stages = catIndexers + [numVector, numScaler, featureVector, algo]\n",
    "        pipeline = Pipeline(stages=stages)\n",
    "        print([type(stage) for stage in pipeline.getStages()])\n",
    "\n",
    "        # Log training parameter values\n",
    "        print (\"Training Linear Regression model...\")\n",
    "        mlflow.log_param('maxIter', algo.getMaxIter())\n",
    "        mlflow.log_param('regParam', algo.getRegParam())\n",
    "        model = pipeline.fit(train)\n",
    "\n",
    "            # Evaluate the model and log metrics\n",
    "        prediction = model.transform(test)\n",
    "        \n",
    "        metrics = [\"r2\", \"mse\", \"rmse\"]\n",
    "        for metric in metrics:\n",
    "            evaluator = RegressionEvaluator(labelCol=\"Sales\", predictionCol=\"prediction\", metricName=metric)\n",
    "            metric_value = evaluator.evaluate(model.transform(test))\n",
    "            print(f\"{metric}: {metric_value}\")\n",
    "            mlflow.log_metric(metric, metric_value)\n",
    "\n",
    "\n",
    "        # Log the model itself\n",
    "        unique_model_name = \"Regressor-\" + str(time.time())\n",
    "        mlflow.spark.log_model(model, unique_model_name, mlflow.spark.get_default_conda_env())\n",
    "        experiment_id = run.info.experiment_id\n",
    "        dbfs_model_path = f\"dbfs:/databricks/mlflow-tracking/{experiment_id}/{run_id}/artifacts/{unique_model_name}\"\n",
    "     \n",
    "        # modelpath = \"/model/%s\" % (unique_model_name)\n",
    "        mlflow.spark.save_model(model, dbfs_model_path)\n",
    "    \n",
    "\n",
    "\n",
    "    print(dbfs_model_path)    \n",
    "    print(\"Experiment run complete.\")\n",
    "    return unique_model_name,run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a94da0a8-269b-4d54-a396-83142fa2e375",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "unique_model_name, run_id=train_market_media_mix_model(train, test, 10, 0.2)\n",
    "print(unique_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8264c466-64a1-482c-a972-190750b55c01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(unique_model_name.split(\".\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9265c3e8-c2b1-42c1-909b-d6d5de448c6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from mlflow.tracking import MlflowClient\n",
    "# import mlflow\n",
    "# client = MlflowClient()\n",
    "\n",
    "# # Register the model\n",
    "# with mlflow.start_run() as run: model_uri = f\"dbfs:/mlflow/{run.info.run_id}/{unique_model_name}\"\n",
    "# # registered_model = client.create_registered_model(unique_model_name.split(\".\")[0])\n",
    "\n",
    "# # Create a model version\n",
    "# model_version = client.create_model_version(\n",
    "#     name=unique_model_name.split(\".\")[0],\n",
    "#     source=model_uri,\n",
    "#     run_id=run_id\n",
    "# )\n",
    "\n",
    "# print(f\"Model registered as {unique_model_name} with version {model_version.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be98574a-cb69-4a78-b547-650ee7de1bcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Model Registering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff41dbb1-e8d2-4c7d-a045-24496414b831",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "import mlflow\n",
    "def register_model(unique_model_name, run_id):\n",
    "    client = MlflowClient()\n",
    "\n",
    "    # Extract model name (remove timestamp suffix)\n",
    "    model_name = unique_model_name.split(\".\")[0]\n",
    "    experiment_id = mlflow.get_experiment_by_name(\"/Users/sruthidas.g@einfochipsindia.onmicrosoft.com/my-experimentdb_train\").experiment_id\n",
    "\n",
    "    # Define DBFS model path\n",
    "    dbfs_model_path = f\"dbfs:/databricks/mlflow-tracking/{experiment_id}/{run_id}/artifacts/{unique_model_name}\"\n",
    "    print(dbfs_model_path)\n",
    "\n",
    "    # Ensure the model exists in MLflow Model Registry\n",
    "    try:\n",
    "        client.get_registered_model(model_name)\n",
    "        print(f\"Model '{model_name}' already exists in MLflow Registry.\")\n",
    "    except:\n",
    "        print(f\"Creating a new registered model '{model_name}'...\")\n",
    "        client.create_registered_model(model_name)\n",
    "\n",
    "    # Create a new model version\n",
    "    model_version = client.create_model_version(\n",
    "        name=model_name,\n",
    "        source=dbfs_model_path,\n",
    "        run_id=run_id\n",
    "    )\n",
    "\n",
    "    print(f\"Model registered as {model_name} with version {model_version.version}\")\n",
    "    return model_name, model_version.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e655827-bea6-4b46-bd9b-a7e482a6e997",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_name, model_version = register_model(unique_model_name, run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79d25348-f580-42e9-a151-bd562d2790fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92de8961-1c55-4ed2-b422-9b9726008d3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Inference Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b02b194e-d70d-4d51-9277-84e7b49f1bbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " %sh\n",
    " rm -r /dbfs/mlflow_lab\n",
    " mkdir /dbfs/mlflow_lab\n",
    " wget -O /dbfs/mlflow_lab/MMM_test.csv https://raw.githubusercontent.com/sruthidasrg/test_databrick/refs/heads/main/MMM_test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6a70013-ffbf-4226-b92d-3c26fb7f58db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "infer_data = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"dbfs:/mlflow_lab//MMM_test.csv\")\n",
    "\n",
    "# List of columns to convert\n",
    "columns_to_convert = [\"TV_Spend\", \"Digital_Spend\", \"Social_Media_Spend\",\"Print_Spend\",\"Discount_Applied\",\"Units_Sold\"]\n",
    "# Apply casting to each column\n",
    "for col_name in columns_to_convert:\n",
    "    infer_data = infer_data.withColumn(col_name, col(col_name).cast(\"long\"))\n",
    "infer_data = infer_data.withColumn(\"Sales\", col(\"Sales\").cast(\"double\"))\n",
    "infer_data = infer_data.withColumn(\"Date\", col(\"Date\").cast(\"date\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efc4f12b-4ff0-410a-a00f-024e936490dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(infer_data.sample(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d1b8448-f595-48b2-9428-ae7a0c068c72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.spark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, MinMaxScaler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "\n",
    "def load_model(model_name, model_version):\n",
    "    \"\"\"Loads the specified model version from MLflow Model Registry.\"\"\"\n",
    "    client = MlflowClient()\n",
    "    model_uri = f\"models:/{model_name}/{model_version}\"\n",
    "    print(f\"Loading model from: {model_uri}\")\n",
    "    model = mlflow.spark.load_model(model_uri)\n",
    "    return model\n",
    "\n",
    "# def preprocess_data(raw_data):\n",
    "#     \"\"\"Apply the same preprocessing steps as in training.\"\"\"\n",
    "#     catFeatures = ['Campaign_Type', 'Product_Type', 'Region']\n",
    "#     numFeatures = ['TV_Spend', 'Digital_Spend', 'Social_Media_Spend', 'Print_Spend', 'Discount_Applied']\n",
    "    \n",
    "#     catIndexers = [StringIndexer(inputCol=col, outputCol=col + \"Idx\", handleInvalid=\"keep\") for col in catFeatures]\n",
    "#     numVector = VectorAssembler(inputCols=numFeatures, outputCol=\"numericFeatures\")\n",
    "#     numScaler = MinMaxScaler(inputCol=\"numericFeatures\", outputCol=\"normalizedFeatures\")\n",
    "#     allFeatures = [col + \"Idx\" for col in catFeatures] + [\"normalizedFeatures\"]\n",
    "#     featureVector = VectorAssembler(inputCols=allFeatures, outputCol=\"features\")\n",
    "    \n",
    "#     stages = catIndexers + [numVector, numScaler, featureVector]\n",
    "    \n",
    "#     from pyspark.ml import Pipeline\n",
    "#     pipeline = Pipeline(stages=stages)\n",
    "#     processed_data = pipeline.fit(raw_data).transform(raw_data)\n",
    "#     return processed_data\n",
    "\n",
    "def make_predictions(model, inference_data):\n",
    "    \"\"\"Runs inference using the loaded model on the given data.\"\"\"\n",
    "    predictions = model.transform(inference_data)\n",
    "    return predictions.select(\"features\", \"prediction\",\"Sales\")\n",
    "\n",
    "def evaluate_rmse(predictions):\n",
    "    \"\"\"Calculates RMSE for model predictions.\"\"\"\n",
    "    evaluator = RegressionEvaluator(labelCol=\"Sales\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "    print(f\"RMSE: {rmse}\")\n",
    "    return rmse\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession.builder.appName(\"MarketMediaMixInference\").getOrCreate()\n",
    "    \n",
    "    # Define model details\n",
    "    # model_name = \"Regressor\"\n",
    "    # model_version = 1  # Update if needed\n",
    "    \n",
    "    # Load the model\n",
    "    model = load_model(model_name, model_version)\n",
    "    \n",
    "  \n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = make_predictions(model, infer_data)\n",
    "    \n",
    "    # Show results\n",
    "    predictions.show()\n",
    "\n",
    "    # Evaluate RMSE\n",
    "    evaluate_rmse(predictions)\n",
    "\n",
    "\n",
    "    \n",
    "    print(\"Inference complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96025e04-2c15-4a8e-89f5-e284e0e671cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "assert train.schema == infer_data.schema, \"Schema mismatch between train and infer_data!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caab94d9-f96f-4abb-9f5e-0477c4d2c122",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a0873d6-226e-497f-978a-a884ce0bdebf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "client = mlflow.tracking.MlflowClient()\n",
    "model_name = unique_model_name.split(\".\")[0]\n",
    "experiment_id = mlflow.get_experiment_by_name(\"/Users/sruthidas.g@einfochipsindia.onmicrosoft.com/my-experimentdb_train\").experiment_id\n",
    "\n",
    "# Define DBFS model path\n",
    "dbfs_model_path = f\"dbfs:/databricks/mlflow-tracking/{experiment_id}/{run_id}/artifacts/{unique_model_name}\"\n",
    "print(dbfs_model_path)\n",
    "client.transition_model_version_stage(\n",
    "    # name=\"my_ml_model\",\n",
    "    name=model_name,       \n",
    "    version=1,\n",
    "    stage=\"Archived\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "674802c7-d58f-4955-9637-de50289aa163",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(train.inputFiles())"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5177109384046576,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) Demo2: MMM_MLflow_Train pipeline_Inference_pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
